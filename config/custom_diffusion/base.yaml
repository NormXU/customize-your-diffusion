name: custom_your_diffusion_base
model:
  type: StableDiffusion
  tokenizer_args:
    type: CLIPTokenizer
  text_model_args:
    type: CLIPTextModel
  pretrained_model_name_or_path: stabilityai/stable-diffusion-2-1
  apply_xformers: false
  unfreeze_unet: crossattn_kv  # crossattn to enable fine-tuning of all key, value or query matrices
  use_ema: false
  train_text_encoder: false
  mixed_precision_flag: "no"   # "["no", "fp16"]
  # wheather to use torch native mix-precision function; accelerator also supports mixed_precision; but be careful, do not use them both
  noise_scheduler:
    type: DDPMScheduler
  placeholder_token: ~
  initializer_token: ktn+pll+ucd  # In all experiments, we initialized the unique modifier token with the token-id 42170
  quantization_type: ~
  model_path: ~

loss:
  prior_loss_weight: 1.0
accelerator:
  # accelertor can override the same settings in native torch
  mixed_precision: "no" # "["no", "fp16", "bf16"]
  # Whether to use mixed precision from accelerator. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >="
  #  " 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the"
  # " flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config."
  gradient_accumulation_steps: 1
  # this is also an accelerator api. somehow improve the efficiency; conflict with my implementation of gradient accumulation
  # if you want to use accelerator gradient accumulation, remember set trainer.grad_accumulate = 1
  # Currently, it's not possible to do gradient accumulation when training two models with accelerate.accumulate which means
  # you cannot use it if you want to train unet and text_encoder simultaneously
  # This will be enabled soon in accelerate. For now, we don't allow gradient accumulation when training two models.
predictor:
  torch_dtype: "fp16"  # "auto"
  height: 512
  width: 512
  sampler:
      type: DPMSolverMultistepScheduler
  prompt: # you can input multiple conditions
    -
  negative_prompt:
    # https://huggingface.co/spaces/stabilityai/stable-diffusion/discussions/7857
    # - ugly, bad anatomy, poorly drawn hands, mutated body parts, blurry image, poorly drawn face
    -
    # - Images cut out at the top, left, right, bottom.
  generated_batch: 5
  save_dir: /home/ysocr/data/workspace/painting/output
  scale: 7.5 # unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))
  num_inference_steps: 50
trainer:
  scale_lr: true  # Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size
  optimizer:
    optimizer_type: "adamw"
    lr: 2.0e-05
    weight_decay: 0.01  # optimizer weight decay
    beta1: 0.9
    beta2: 0.999
    eps: 1.0e-8
  scheduler:
    scheduler_type: "constant"
    # warmup_epochs: 1
    warmup_steps: 10
  epochs: 100
  max_train_steps: 500
  grad_clip: 1.0
  save_dir: /home/ysocr/data/workspace/custom_diffusion
  tensorboard_dir: /home/ysocr/data/workspace/custom_diffusion/tensorboard
  save_best: false
  save_epoch_freq: 10
  save_step_freq: 200
  print_freq: 10
  grad_accumulate: 1 # gradient accumulation
  random_seed: ~
datasets:
  train:
    dataset:
      type: CustomDiffusionDataset
      size: 512
      center_crop: false
      hflip: false
      with_prior_preservation: false
      num_class_images: 10
      concepts_list:
        - instance_prompt: <V>
          use_default_template: true
          class_prompt: photo of a cat-like toy
          instance_data_dir: /home/data/ugly-cat
          class_data_dir: /home/data/generated_data/cat_toy
    num_workers: 0
    batch_size: 2
    collate_fn:
      type: CustomDiffusionCollectFn