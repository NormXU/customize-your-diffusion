# These parameters work pretty well. Use it as a reference
# do NOT change unless you have better configurations
name: dream_booth_base
base: config/custom_diffusion/base.yaml
model:
  unfreeze_unet: all
  train_text_encoder: false
  placeholder_token: ~
  model_path: ~

accelerator:
  # accelertor can override the same settings in native torch
  mixed_precision: "no" # "["no", "fp16", "bf16"]
  # Whether to use mixed precision from accelerator. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >="
  #  " 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the"
  # " flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config."
  gradient_accumulation_steps: 1
  # this is also an accelerator api. somehow improve the efficiency; conflict with my implementation of gradient accumulation
  # if you want to use accelerator gradient accumulation, remember set trainer.grad_accumulate = 1
  # Currently, it's not possible to do gradient accumulation when training two models with accelerate.accumulate which means
  # you cannot use it if you want to train unet and text_encoder simultaneously
  # This will be enabled soon in accelerate. For now, we don't allow gradient accumulation when training two models.
predictor:
  prompt: # you can input multiple conditions
    - a painting of a <ugly-cat> backpack
  negative_prompt:
    # - Images cut out at the top, left, right, bottom.
    -
  generated_batch: 5
  save_dir: /home/workspace/painting/output
  scale: 7.5 # unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))
  num_inference_steps: 50
trainer:
  scale_lr: true  # Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size
  optimizer:
    optimizer_type: "adamw"
    lr: 2.0e-06
  scheduler:
    scheduler_type: "constant"
    # warmup_epochs: 1
    warmup_steps: 10
  epochs: 200
  max_train_steps: 1000
  grad_clip: 1.0
  save_dir: /home/workspace/dream_booth
  tensorboard_dir: /home/workspace/dream_booth/tensorboard
  save_best: false
  save_epoch_freq: 10
  save_step_freq: 200
  print_freq: 10
  grad_accumulate: 1 # gradient accumulation
  random_seed: ~
datasets:
  train:
    dataset:
      type: CustomDiffusionDataset
      size: 512
      center_crop: false
      hflip: false
      with_prior_preservation: false
      num_class_images: 10
      concepts_list:
        - instance_prompt: <ugly-cat>
          use_default_template: true
          class_prompt: photo of a cat-like toy
          instance_data_dir: /path/to/dreambooth/images
          class_data_dir: /path/to/normalization/images
    num_workers: 0
    batch_size: 2
    collate_fn:
      type: CustomDiffusionCollectFn